{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "37af7ffa",
   "metadata": {},
   "source": [
    "# Image Captioning project\n",
    "Build our first prototype model for Image captioning using structure combination of CNN and Transformer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f916c16a",
   "metadata": {},
   "outputs": [],
   "source": [
    "name_1 = 'Qinghuan Liu'\n",
    "name_2 = 'Jingkai Zhou'\n",
    "name_3 = 'Chang Li'\n",
    "naem_4 = 'Yawen Liu'\n",
    "planning_group = '28'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "897a7210",
   "metadata": {},
   "source": [
    "---\n",
    "## 0. Import\n",
    "import possible files here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf5757e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ca572b3",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Load dataset\n",
    "In this part, load and preprocess famous captioning dataset ['Flickr 8k Dataset'](https://www.kaggle.com/datasets/adityajn105/flickr8k). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02912247",
   "metadata": {},
   "source": [
    "---\n",
    "### 1.1 Load dataset from disk and split into training / validation / test sets\n",
    "Since the size of our trainning archive is not relatively large, the ratio is chosen according to the technique [Data splitting technique to fit any Machine Learning Model](https://towardsdatascience.com/data-splitting-technique-to-fit-any-machine-learning-model-c0d7f3f1c790)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65e45686",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "# Some file system operation are not covered by 'Path' and we use 'shutil' for that\n",
    "import shutil\n",
    "\n",
    "# Regular expressions are used to find patterns in strings\n",
    "import re\n",
    "\n",
    "# For splitting the data\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "930e5f91",
   "metadata": {},
   "source": [
    "Statistical information on dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddc13be9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to Flickr8K_ photos\n",
    "path_Flickr_jpg = \"G:/GitHub/Deep_Learning_Proj/archive/Images\"\n",
    "# Path to caption file\n",
    "path_Flickr_text = \"G:/GitHub/Deep_Learning_Proj/archive/captions.txt\"\n",
    "\n",
    "image_all = Path.cwd() / \"G:/GitHub/Deep_Learning_Proj/archive/Images/\"\n",
    "\n",
    "all_image_filenames = list(image_all.glob(\"*.jpg\"))\n",
    "\n",
    "print(len(all_image_filenames))\n",
    "print(all_image_filenames[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4afc3b32",
   "metadata": {},
   "source": [
    "split data with predefined ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "35fae265",
   "metadata": {},
   "outputs": [],
   "source": [
    "split_ratio_dataset = 0.2\n",
    "\n",
    "image_train, image_val = \\\n",
    "train_test_split(all_image_filenames,  \n",
    "              test_size = split_ratio_dataset,\n",
    "              random_state = 2)\n",
    "\n",
    "# Following operations create train and val dataset folders located at the same root as this proj.ipynb\n",
    "subdirectories = {\"./image_train\": image_train,\n",
    "                 \"./image_val\": image_val\n",
    "                 }\n",
    "\n",
    "for subdirectory in subdirectories.keys():\n",
    "    subdirectory = Path(subdirectory)\n",
    "    subdirectory.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    \n",
    "# Put the training and validation data in the respective folders\n",
    "def fill_sub_dir(sub_dir, file_subset):\n",
    "    \"\"\"This function copies files from the `train_all` to a `<sub_dir>`\n",
    "    A more efficient solution would be to use \"symbolic links\" (see https://kb.iu.edu/d/abbe)\n",
    "    but for simplicity hard copies is used instead.\n",
    "    \"\"\"\n",
    "    for file in file_subset:\n",
    "        file_path = Path.cwd() / sub_dir / file.name\n",
    "        shutil.copyfile(file, file_path)\n",
    "        \n",
    "for sub_dir, file_subset in subdirectories.items():\n",
    "    fill_sub_dir(sub_dir, file_subset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aed8fabf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6472\n",
      "1619\n"
     ]
    }
   ],
   "source": [
    "print(len(image_train))\n",
    "print(len(image_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bde7b8c",
   "metadata": {},
   "source": [
    "Point to training and validation path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79936e65",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = \"./image_train/\"\n",
    "val_path = \"./image_val/\"\n",
    "caption_path = \"./captions.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4d8d5e7",
   "metadata": {},
   "source": [
    "## 1.2 Import image classes\n",
    "Here, as what we did before in HA1, new useful classes are created in another file 'data_loader.py'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ded461c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports \n",
    "import os\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import spacy\n",
    "import torch\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import DataLoader,Dataset\n",
    "import torchvision.transforms as T\n",
    "from PIL import Image\n",
    "\n",
    "class Vocabulary:\n",
    "    #tokenizer\n",
    "    spacy_eng = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "    \n",
    "    def __init__(self,freq_threshold):\n",
    "        #setting the pre-reserved tokens int to string tokens\n",
    "        self.itos = {0:\"<PAD>\",1:\"<SOS>\",2:\"<EOS>\",3:\"<UNK>\"}\n",
    "        \n",
    "        #string to int tokens\n",
    "        #its reverse dict self.itos\n",
    "        self.stoi = {v:k for k,v in self.itos.items()}\n",
    "        \n",
    "        self.freq_threshold = freq_threshold\n",
    "        \n",
    "        \n",
    "        \n",
    "    def __len__(self): return len(self.itos)\n",
    "    \n",
    "    @staticmethod\n",
    "    def tokenize(text):\n",
    "        return [token.text.lower() for token in Vocabulary.spacy_eng.tokenizer(text)]\n",
    "    \n",
    "    def build_vocab(self, sentence_list):\n",
    "        frequencies = Counter()\n",
    "        \n",
    "        #staring index 4\n",
    "        idx = 4\n",
    "        \n",
    "        for sentence in sentence_list:\n",
    "            for word in self.tokenize(sentence):\n",
    "                frequencies[word] += 1\n",
    "                \n",
    "                #add the word to the vocab if it reaches minum frequecy threshold\n",
    "                if frequencies[word] == self.freq_threshold:\n",
    "                    self.stoi[word] = idx\n",
    "                    self.itos[idx] = word\n",
    "                    idx += 1\n",
    "    \n",
    "    def numericalize(self,text):\n",
    "        \"\"\" For each word in the text corresponding index token for that word form the vocab built as list \"\"\"\n",
    "        tokenized_text = self.tokenize(text)\n",
    "        return [ self.stoi[token] if token in self.stoi else self.stoi[\"<UNK>\"] for token in tokenized_text ]    \n",
    "    \n",
    "class FlickrDataset(Dataset):\n",
    "    \"\"\"\n",
    "    FlickrDataset\n",
    "    \"\"\"\n",
    "    def __init__(self,root_dir,caption_file,transform=None,freq_threshold=5):\n",
    "        self.root_dir = root_dir\n",
    "        self.df = pd.read_csv(caption_file)\n",
    "        self.transform = transform\n",
    "        \n",
    "        #Get image and caption colum from the dataframe\n",
    "        self.imgs = self.df[\"image\"]\n",
    "        self.captions = self.df[\"caption\"]\n",
    "        \n",
    "        #Initialize vocabulary and build vocab\n",
    "        self.vocab = Vocabulary(freq_threshold)\n",
    "        self.vocab.build_vocab(self.captions.tolist())\n",
    "        \n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self,idx):\n",
    "        caption = self.captions[idx]\n",
    "        img_name = self.imgs[idx]\n",
    "        img_location = os.path.join(self.root_dir,img_name)\n",
    "        img = Image.open(img_location).convert(\"RGB\")\n",
    "        \n",
    "        #apply the transfromation to the image\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "        \n",
    "        #numericalize the caption text\n",
    "        caption_vec = []\n",
    "        caption_vec += [self.vocab.stoi[\"<SOS>\"]]\n",
    "        caption_vec += self.vocab.numericalize(caption)\n",
    "        caption_vec += [self.vocab.stoi[\"<EOS>\"]]\n",
    "        \n",
    "        return img, torch.tensor(caption_vec)\n",
    "\n",
    "class CapsCollate:\n",
    "    \"\"\"\n",
    "    Collate to apply the padding to the captions with dataloader\n",
    "    \"\"\"\n",
    "    def __init__(self,pad_idx,batch_first=False):\n",
    "        self.pad_idx = pad_idx\n",
    "        self.batch_first = batch_first\n",
    "    \n",
    "    def __call__(self,batch):\n",
    "        imgs = [item[0].unsqueeze(0) for item in batch]\n",
    "        imgs = torch.cat(imgs,dim=0)\n",
    "        \n",
    "        targets = [item[1] for item in batch]\n",
    "        targets = pad_sequence(targets, batch_first=self.batch_first, padding_value=self.pad_idx)\n",
    "        return imgs,targets\n",
    "\n",
    "\n",
    "def get_data_loader(dataset,batch_size,shuffle=False,num_workers=1):\n",
    "    \"\"\"\n",
    "    Returns torch dataloader for the flicker8k dataset\n",
    "    \n",
    "    Parameters\n",
    "    -----------\n",
    "    dataset: FlickrDataset\n",
    "        custom torchdataset named FlickrDataset \n",
    "    batch_size: int\n",
    "        number of data to load in a particular batch\n",
    "    shuffle: boolean,optional;\n",
    "        should shuffle the datasests (default is False)\n",
    "    num_workers: int,optional\n",
    "        numbers of workers to run (default is 1)  \n",
    "    \"\"\"\n",
    "\n",
    "    pad_idx = dataset.vocab.stoi[\"<PAD>\"]\n",
    "    collate_fn = CapsCollate(pad_idx=pad_idx,batch_first=True)\n",
    "\n",
    "    data_loader = DataLoader(\n",
    "        dataset=dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        num_workers=num_workers,\n",
    "        collate_fn=collate_fn\n",
    "    )\n",
    "\n",
    "    return data_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d9da4cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports \n",
    "\n",
    "#reading the training text data \n",
    "import pandas as pd\n",
    "caption_file = './cap_train.txt'\n",
    "df = pd.read_csv(caption_file)\n",
    "print(\"There are {} train captions\".format(len(df)))\n",
    "df.head(7)\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader,Dataset\n",
    "import torchvision.transforms as T\n",
    "\n",
    "#custom imports \n",
    "# from data_loader import FlickrDataset,get_data_loader\n",
    "\n",
    "\n",
    "#setting the constants\n",
    "BATCH_SIZE = 128\n",
    "NUM_WORKER = 0\n",
    "\n",
    "#defining the transform to be applied\n",
    "transforms = T.Compose([\n",
    "    T.Resize(256),                     \n",
    "    T.RandomCrop(224),                 \n",
    "    T.ToTensor(),                               \n",
    "    T.Normalize((0.485, 0.456, 0.406),(0.229, 0.224, 0.225))\n",
    "])\n",
    "\n",
    "\n",
    "#testing the dataset class\n",
    "dataset =  FlickrDataset(\n",
    "    root_dir = './image_train/',\n",
    "    caption_file = './cap_train.txt',\n",
    "    transform=transforms\n",
    ")\n",
    "\n",
    "#writing the dataloader\n",
    "data_loader = get_data_loader(\n",
    "    dataset=dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    num_workers=NUM_WORKER,\n",
    "    shuffle=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98abe771",
   "metadata": {},
   "source": [
    "## 2 Build learning model\n",
    "In this part, learning model should be built for. And test it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "029eda9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "\n",
    "class EncoderCNN(nn.Module):\n",
    "    def __init__(self, embed_size):\n",
    "        super(EncoderCNN, self).__init__()\n",
    "        resnet = models.resnet50(pretrained=True)\n",
    "        for param in resnet.parameters():\n",
    "            param.requires_grad_(False)\n",
    "        \n",
    "        modules = list(resnet.children())[:-1]\n",
    "        self.resnet = nn.Sequential(*modules)\n",
    "        self.embed = nn.Linear(resnet.fc.in_features, embed_size)\n",
    "\n",
    "    def forward(self, images):\n",
    "        features = self.resnet(images)\n",
    "        features = features.view(features.size(0), -1)\n",
    "        features = self.embed(features)\n",
    "        return features\n",
    "\n",
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self,embed_size, hidden_size, vocab_size, num_layers=1,drop_prob=0.3):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size,embed_size)\n",
    "        self.lstm = nn.LSTM(embed_size,hidden_size,num_layers=num_layers,batch_first=True)\n",
    "        self.fcn = nn.Linear(hidden_size,vocab_size)\n",
    "        self.drop = nn.Dropout(drop_prob)\n",
    "    \n",
    "    def forward(self, features, captions):\n",
    "        \n",
    "        #vectorize the caption\n",
    "        embeds = self.embedding(captions[:,:-1])\n",
    "        \n",
    "        #concat the features and captions\n",
    "        x = torch.cat((features.unsqueeze(1),embeds),dim=1) \n",
    "        x,_ = self.lstm(x)\n",
    "        x = self.fcn(x)\n",
    "        return x\n",
    "    \n",
    "    def generate_caption(self,inputs,hidden=None,max_len=20,vocab=None):\n",
    "        # Inference part\n",
    "        # Given the image features generate the captions\n",
    "        \n",
    "        batch_size = inputs.size(0)\n",
    "        \n",
    "        captions = []\n",
    "        \n",
    "        for i in range(max_len):\n",
    "            output,hidden = self.lstm(inputs,hidden)\n",
    "            output = self.fcn(output)\n",
    "            output = output.view(batch_size,-1)\n",
    "        \n",
    "            \n",
    "            #select the word with most val\n",
    "            predicted_word_idx = output.argmax(dim=1)\n",
    "            \n",
    "            #save the generated word\n",
    "            captions.append(predicted_word_idx.item())\n",
    "            \n",
    "            #end if <EOS detected>\n",
    "            if vocab.itos[predicted_word_idx.item()] == \"<EOS>\":\n",
    "                break\n",
    "            \n",
    "            #send generated word as the next caption\n",
    "            inputs = self.embedding(predicted_word_idx.unsqueeze(0))\n",
    "        \n",
    "        #covert the vocab idx to words and return sentence\n",
    "        return [vocab.itos[idx] for idx in captions]\n",
    "        \n",
    "            \n",
    "class EncoderDecoder(nn.Module):\n",
    "    def __init__(self,embed_size, hidden_size, vocab_size, num_layers=1,drop_prob=0.3):\n",
    "        super().__init__()\n",
    "        self.encoder = EncoderCNN(embed_size)\n",
    "        self.decoder = DecoderRNN(embed_size,hidden_size,vocab_size,num_layers,drop_prob)\n",
    "    \n",
    "    def forward(self, images, captions):\n",
    "        features = self.encoder(images)\n",
    "        outputs = self.decoder(features, captions)\n",
    "        return outputs\n",
    "    \n",
    "    \n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "082eac9f",
   "metadata": {},
   "source": [
    "### 2.1 Setting hyperparams for training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d418bfd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "embed_size = 400\n",
    "hidden_size = 512\n",
    "vocab_size = len(dataset.vocab)\n",
    "num_layers = 2\n",
    "learning_rate = 0.0001\n",
    "num_epochs = 2\n",
    "\n",
    "# initialize model, loss etc\n",
    "model = EncoderDecoder(embed_size, hidden_size, vocab_size, num_layers).to(device)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=dataset.vocab.stoi[\"<PAD>\"])\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "\n",
    "\n",
    "num_epochs = 2\n",
    "print_every = 1\n",
    "\n",
    "for epoch in range(1,num_epochs+1):   \n",
    "    for idx, (image, captions) in enumerate(iter(data_loader)):\n",
    "        image,captions = image.to(device),captions.to(device)\n",
    "\n",
    "        # Zero the gradients.\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Feed forward\n",
    "        outputs = model(image, captions)\n",
    "        \n",
    "        # Calculate the batch loss.\n",
    "        loss = criterion(outputs.view(-1, vocab_size), captions.view(-1))\n",
    "\n",
    "        \n",
    "        # Backward pass.\n",
    "        loss.backward()\n",
    "\n",
    "        # Update the parameters in the optimizer.\n",
    "        optimizer.step()\n",
    "        \n",
    "        if (idx+1)%print_every == 0:\n",
    "            print(\"Epoch: {} loss: {:.5f}\".format(epoch,loss.item()))\n",
    "            \n",
    "            \n",
    "            #generate the caption\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                dataiter = iter(data_loader)\n",
    "                img,_ = next(dataiter)\n",
    "                features = model.encoder(img[0:1].to(device))\n",
    "                caps = model.decoder.generate_caption(features.unsqueeze(0),vocab=dataset.vocab)\n",
    "                caption = ' '.join(caps)\n",
    "                show_image(img[0],title=caption)\n",
    "                \n",
    "            model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6470519a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
